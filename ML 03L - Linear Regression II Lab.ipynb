{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec106333-51aa-4973-8e56-a8b16fa29e7b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35c53184-008b-4a75-a0cb-1ea173e170b2",
     "showTitle": false,
     "title": "--i18n-4e1b9835-762c-42f2-9ff8-75164cb1a702"
    }
   },
   "source": [
    "# Linear Regression II Lab\n",
    "\n",
    "Alright! We're making progress. Still not a great RMSE or R2, but better than the baseline or just using a single feature.\n",
    "\n",
    "In the lab, you will see how to improve our performance even more.\n",
    "\n",
    "## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Learning Objectives:<br>\n",
    "\n",
    "By the end of this lab, you should be able to;\n",
    "\n",
    "* Use RFormula to simplify the process of using StringIndexer, OneHotEncoder, and VectorAssembler\n",
    "* Transform data into log scale to fit a model\n",
    "* Convert log scale predictions to appropriate form for model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55587dfb-3c94-434b-9792-ff8d53f96d1f",
     "showTitle": false,
     "title": "--i18n-ad45216b-38b0-401e-adf7-fb42ace220e2"
    }
   },
   "source": [
    "## Lab Setup\n",
    "\n",
    "\n",
    "The first thing we're going to do is to **run setup script**. This script will define the required configuration variables that are scoped to each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad0fd3d9-ba04-4626-a354-d27c65371b43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48bda207-321c-48ce-bd4e-2d8f9317dac6",
     "showTitle": false,
     "title": "--i18n-1500312a-d027-42d0-a787-0dea4f8d7d03"
    }
   },
   "source": [
    "## Load Dataset and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a71a821-8f91-46e3-b161-71bc2e458f44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path = f\"{DA.paths.datasets}/airbnb/sf-listings/sf-listings-2019-03-06-clean.delta/\"\n",
    "airbnb_df = spark.read.format(\"delta\").load(file_path)\n",
    "train_df, test_df = airbnb_df.randomSplit([.8, .2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24fca4d9-83b3-47e4-9a00-f204d1327f3f",
     "showTitle": false,
     "title": "--i18n-a427d25c-591f-4899-866a-14064eff40e3"
    }
   },
   "source": [
    "## RFormula\n",
    "\n",
    "Instead of manually specifying which columns are categorical to the StringIndexer and OneHotEncoder, <a href=\"(https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.RFormula.html?highlight=rformula#pyspark.ml.feature.RFormula\" target=\"_blank\">RFormula</a> can do that automatically for you.\n",
    "\n",
    "With RFormula, if you have any columns of type String, it treats it as a categorical feature and string indexes & one hot encodes it for us. Otherwise, it leaves as it is. Then it combines all of one-hot encoded features and numeric features into a single vector, called **`features`**.\n",
    "\n",
    "You can see a detailed example of how to use RFormula <a href=\"https://spark.apache.org/docs/latest/ml-features.html#rformula\" target=\"_blank\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "134c398f-32e7-4749-ae90-3b28289de106",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RFormula\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "r_formula = RFormula(formula=\"price ~ .\", featuresCol=\"features\", labelCol=\"price\", handleInvalid=\"skip\") # Look at handleInvalid\n",
    "\n",
    "lr = LinearRegression(labelCol=\"price\", featuresCol=\"features\")\n",
    "pipeline = Pipeline(stages=[r_formula, lr])\n",
    "pipeline_model = pipeline.fit(train_df)\n",
    "pred_df = pipeline_model.transform(test_df)\n",
    "\n",
    "regression_evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\")\n",
    "rmse = regression_evaluator.setMetricName(\"rmse\").evaluate(pred_df)\n",
    "r2 = regression_evaluator.setMetricName(\"r2\").evaluate(pred_df)\n",
    "print(f\"RMSE is {rmse}\")\n",
    "print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6948e34a-d3e3-4562-87f2-fc9ebe6e92bf",
     "showTitle": false,
     "title": "--i18n-c9898a31-90e4-4a6d-87e6-731b95c764bd"
    }
   },
   "source": [
    "## Log Scale\n",
    "\n",
    "Now that we have verified we get the same result using RFormula as above, we are going to improve upon our model. If you recall, our price dependent variable appears to be log-normally distributed, so we are going to try to predict it on the log scale.\n",
    "\n",
    "Let's convert our price to be on log scale, and have the linear regression model predict the log price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6dbaf25-689b-4a27-8cdc-fd3151e28612",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import log\n",
    "\n",
    "display(train_df.select(log(\"price\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c236efba-844d-4279-a220-2498c2c0cc9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "from pyspark.sql.functions import col, log\n",
    "\n",
    "log_train_df = train_df.withColumn(\"log_price\", log(col(\"price\")))\n",
    "log_test_df = test_df.withColumn(\"log_price\", log(col(\"price\")))\n",
    "\n",
    "r_formula = RFormula(formula=\"log_price ~ . - price\", featuresCol=\"features\", labelCol=\"log_price\", handleInvalid=\"skip\") \n",
    "\n",
    "lr.setLabelCol(\"log_price\").setPredictionCol(\"log_pred\")\n",
    "pipeline = Pipeline(stages=[r_formula, lr])\n",
    "pipeline_model = pipeline.fit(log_train_df)\n",
    "pred_df = pipeline_model.transform(log_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e58dd187-68a4-4f0f-b293-d492c5eae283",
     "showTitle": false,
     "title": "--i18n-51b5e35f-e527-438a-ab56-2d4d0d389d29"
    }
   },
   "source": [
    "## Exponentiate\n",
    "\n",
    "In order to interpret our RMSE, we need to convert our predictions back from logarithmic scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be0d6ad8-d168-4b63-94d5-57aae05e7adf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "from pyspark.sql.functions import col, exp\n",
    "\n",
    "exp_df = pred_df.withColumn(\"prediction\", exp(col(\"log_pred\")))\n",
    "\n",
    "rmse = regression_evaluator.setMetricName(\"rmse\").evaluate(exp_df)\n",
    "r2 = regression_evaluator.setMetricName(\"r2\").evaluate(exp_df)\n",
    "print(f\"RMSE is {rmse}\")\n",
    "print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af9266d0-eb15-4268-945d-fb6393ae5ba5",
     "showTitle": false,
     "title": "--i18n-05d3baa6-bb71-4c31-984b-a2daabc35f97"
    }
   },
   "source": [
    "Nice job! You have increased the R2 and dropped the RMSE significantly in comparison to the previous model.\n",
    "\n",
    "In the next few notebooks, we will see how we can reduce the RMSE even more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ce2dff8-6b11-4d71-b291-c89ce80c86a8",
     "showTitle": false,
     "title": "--i18n-a2c7fb12-fd0b-493f-be4f-793d0a61695b"
    }
   },
   "source": [
    "## Classroom Cleanup\n",
    "\n",
    "Run the following cell to remove lessons-specific assets created during this lesson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26f0d8d7-f700-4840-9409-90642710e52a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4f35c66-7067-4257-ab1a-43181d266448",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "&copy; 2023 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ML 03L - Linear Regression II Lab",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
