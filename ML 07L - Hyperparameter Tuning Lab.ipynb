{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bf87c77-3e0a-4564-aaeb-12d3f4aad473",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ddbf7ff-3498-4128-a2e2-aa6d3506ab96",
     "showTitle": false,
     "title": "--i18n-b778c8d0-84e6-4192-a921-b9b60fd20d9b"
    }
   },
   "source": [
    "# Hyperparameter Tuning with Random Forests\n",
    "\n",
    "In this lab, you will convert the Airbnb problem to a classification dataset, build a random forest classifier, and tune some hyperparameters of the random forest.\n",
    "\n",
    "## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Learning Objectives:<br>\n",
    "\n",
    "By the end of this lab, you should be able to;\n",
    "\n",
    "* Perform grid search on a random forest based model\n",
    "* Generate feature importance scores and classification metrics for a random forest model\n",
    "* Identify differences between scikit-learn's and Spark ML's Random Forest implementation\n",
    "\n",
    "\n",
    " \n",
    "You can read more about the distributed implementation of Random Forests in the Spark <a href=\"https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/tree/impl/RandomForest.scala#L42\" target=\"_blank\">source code</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7524ae08-8139-42ee-8e29-02ea50225fba",
     "showTitle": false,
     "title": "--i18n-40dbd041-1a78-4f06-b83f-e9ad3d51d6ed"
    }
   },
   "source": [
    "## Lab Setup\n",
    "\n",
    "The first thing we're going to do is to **run setup script**. This script will define the required configuration variables that are scoped to each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b80e052-ef99-41f7-bab1-e731484eb5d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nPython interpreter will be restarted.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the learning environment:\n| No action taken\n\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/scalable-machine-learning-with-apache-spark/v02\"\n\nValidating the locally installed datasets:\n| listing local files...(2 seconds)\n| validation completed...(2 seconds total)\n\nCreating & using the schema \"odl_user_1002406_0svy_da_sml\" in the catalog \"hive_metastore\"...(1 seconds)\n\nPredefined tables in \"odl_user_1002406_0svy_da_sml\":\n| -none-\n\nPredefined paths variables:\n| DA.paths.working_dir: dbfs:/mnt/dbacademy-users/odl_user_1002406@databrickslabs.com/scalable-machine-learning-with-apache-spark\n| DA.paths.user_db:     dbfs:/mnt/dbacademy-users/odl_user_1002406@databrickslabs.com/scalable-machine-learning-with-apache-spark/database.db\n| DA.paths.datasets:    dbfs:/mnt/dbacademy-datasets/scalable-machine-learning-with-apache-spark/v02\n\nSetup completed (9 seconds)\n"
     ]
    }
   ],
   "source": [
    "%run \"../Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b331672c-d385-4c35-b58d-86a1b3b522dd",
     "showTitle": false,
     "title": "--i18n-02dc0920-88e1-4f5b-886c-62b8cc02d1bb"
    }
   },
   "source": [
    "## From Regression to Classification\n",
    "\n",
    "In this case, we'll turn the Airbnb housing dataset into a classification problem to **classify between high and low price listings.**  Our **`class`** column will be:<br><br>\n",
    "\n",
    "- **`0`** for a low cost listing of under $150\n",
    "- **`1`** for a high cost listing of $150 or more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bae0340-b11a-4da3-99ce-d01dab520b81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "file_path = f\"{DA.paths.datasets}/airbnb/sf-listings/sf-listings-2019-03-06-clean.delta/\"\n",
    "\n",
    "airbnb_df = (spark\n",
    "            .read\n",
    "            .format(\"delta\")\n",
    "            .load(file_path)\n",
    "            .withColumn(\"priceClass\", (col(\"price\") >= 150).cast(\"int\"))\n",
    "            .drop(\"price\")\n",
    "           )\n",
    "\n",
    "train_df, test_df = airbnb_df.randomSplit([.8, .2], seed=42)\n",
    "\n",
    "categorical_cols = [field for (field, dataType) in train_df.dtypes if dataType == \"string\"]\n",
    "index_output_cols = [x + \"Index\" for x in categorical_cols]\n",
    "\n",
    "string_indexer = StringIndexer(inputCols=categorical_cols, outputCols=index_output_cols, handleInvalid=\"skip\")\n",
    "\n",
    "numeric_cols = [field for (field, dataType) in train_df.dtypes if ((dataType == \"double\") & (field != \"priceClass\"))]\n",
    "assembler_inputs = index_output_cols + numeric_cols\n",
    "vec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "737f3413-98b0-419d-9da3-8ee43bd4c6c4",
     "showTitle": false,
     "title": "--i18n-e3bb8033-43ea-439c-a134-36bedbeff408"
    }
   },
   "source": [
    "### Why can't we OHE?\n",
    "\n",
    "**Question:** What would go wrong if we One Hot Encoded our variables before passing them into the random forest?\n",
    "\n",
    "**HINT:** Think about what would happen to the \"randomness\" of feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7eb3bbda-02fc-4831-9c47-141044b86802",
     "showTitle": false,
     "title": "--i18n-0e9bdc2f-0d8d-41cb-9509-47833d66bc5e"
    }
   },
   "source": [
    "## Random Forest\n",
    "\n",
    "Create a Random Forest classifer called **`rf`** with the **`labelCol=priceClass`**, **`maxBins=40`**, and **`seed=42`** (for reproducibility).\n",
    "\n",
    "It's under **`pyspark.ml.classification.RandomForestClassifier`** in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fb58ca5-3abb-4561-b3a7-af1e21d3545a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"priceClass\", maxBins=40, seed=42)\n",
    "stages =[string_indexer,vec_assembler,rf]\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a3f53c6-0bc2-415c-8a5e-2e9866b3b18c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Params.explainParams of RandomForestClassifier_22accd7f09d3>\n"
     ]
    }
   ],
   "source": [
    "print(rf.explainParams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a90778fd-2b95-4a28-b865-7f01b19aeebe",
     "showTitle": false,
     "title": "--i18n-7f3962e7-51b8-4477-9599-2465ab94a049"
    }
   },
   "source": [
    "## Grid Search\n",
    "\n",
    "There are a lot of hyperparameters we could tune, and it would take a long time to manually configure.\n",
    "\n",
    "Let's use Spark's <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.ParamGridBuilder.html?highlight=paramgrid#pyspark.ml.tuning.ParamGridBuilder\" target=\"_blank\">ParamGridBuilder</a> to find the optimal hyperparameters in a more systematic approach. Call this variable **`param_grid`**.\n",
    "\n",
    "Let's define a grid of hyperparameters to test:\n",
    "  - maxDepth: max depth of the decision tree (Use the values **`2, 5, 10`**)\n",
    "  - numTrees: number of decision trees (Use the values **`10, 20, 100`**)\n",
    "\n",
    "**`addGrid()`** accepts the name of the parameter (e.g. **`rf.maxDepth`**), and a list of the possible values (e.g. **`[2, 5, 10]`**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14e6c696-041c-4fcf-872b-577de6580d23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "parm_grid = (ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, [2,5,10])\n",
    "              .addGrid(rf.numTrees, [10,20,100])\n",
    "              .build())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a48df1dd-07b1-401b-ac77-83fed2b5dcf3",
     "showTitle": false,
     "title": "--i18n-e1862bae-e31e-4f5a-ab0e-926261c4e27b"
    }
   },
   "source": [
    "## Evaluator\n",
    "\n",
    "In the past, we used a **`RegressionEvaluator`**.  For classification, we can use a <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.BinaryClassificationEvaluator.html?highlight=binaryclass#pyspark.ml.evaluation.BinaryClassificationEvaluator\" target=\"_blank\">BinaryClassificationEvaluator</a> if we have two classes or <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.MulticlassClassificationEvaluator.html?highlight=multiclass#pyspark.ml.evaluation.MulticlassClassificationEvaluator\" target=\"_blank\">MulticlassClassificationEvaluator</a> for more than two classes.\n",
    "\n",
    "Create a **`BinaryClassificationEvaluator`** with **`areaUnderROC`** as the metric.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"/> <a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\" target=\"_blank\">Read more on ROC curves here.</a>  In essence, it compares true positive and false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9c9986c-3393-473d-a658-7ed22059ea78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"priceClass\", metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "721d5fb4-7f0e-4dd8-8800-fbf54b852789",
     "showTitle": false,
     "title": "--i18n-ea1c0e11-125d-4067-bd70-0bd6c7ca3cdb"
    }
   },
   "source": [
    "## Cross Validation\n",
    "\n",
    "We are going to do 3-Fold cross-validation and set the **`seed`**=42 on the cross-validator for reproducibility.\n",
    "\n",
    "Put the Random Forest in the CV to speed up the <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.CrossValidator.html?highlight=crossvalidator#pyspark.ml.tuning.CrossValidator\" target=\"_blank\">cross validation</a> (as opposed to the pipeline in the CV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78fe6c76-4056-450c-821a-f525a3320d14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "cv = CrossValidator(estimator=rf, evaluator=evaluator, estimatorParamMaps=parm_grid,numFolds=3,seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5f23208-cca8-4635-886a-85bc8a3a9cae",
     "showTitle": false,
     "title": "--i18n-1f8cebd5-673c-4513-b73b-b64b0a56297c"
    }
   },
   "source": [
    "## Pipeline\n",
    "\n",
    "Let's fit the pipeline with our cross validator to our training data (this may take a few minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac881910-e50b-4534-88fd-99fc6ab43907",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stages = [string_indexer, vec_assembler, cv]\n",
    "\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "pipeline_model = pipeline.fit(train_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62673c91-9776-49ea-a6a4-e750dabd32e0",
     "showTitle": false,
     "title": "--i18n-70cdbfa3-0dd7-4f23-b755-afc0dadd7eb2"
    }
   },
   "source": [
    "## Hyperparameter\n",
    "\n",
    "Which hyperparameter combination performed the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28998d3d-261f-4e11-b72f-d9a41646c0a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[24]: [({Param(parent='RandomForestClassifier_22accd7f09d3', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 2,\n   Param(parent='RandomForestClassifier_22accd7f09d3', name='numTrees', doc='Number of trees to train (>= 1).'): 10},\n  0.8494609892340327),\n ({Param(parent='RandomForestClassifier_22accd7f09d3', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 2,\n   Param(parent='RandomForestClassifier_22accd7f09d3', name='numTrees', doc='Number of trees to train (>= 1).'): 20},\n  0.8450403538026398),\n ({Param(parent='RandomForestClassifier_22accd7f09d3', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 2,\n   Param(parent='RandomForestClassifier_22accd7f09d3', name='numTrees', doc='Number of trees to train (>= 1).'): 100},\n  0.857236821089423),\n ({Param(parent='RandomForestClassifier_22accd7f09d3', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5,\n   Param(parent='RandomForestClassifier_22accd7f09d3', name='numTrees', doc='Number of trees to train (>= 1).'): 10},\n  0.8794946343548472),\n ({Param(parent='RandomForestClassifier_22accd7f09d3', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5,\n   Param(parent='RandomForestClassifier_22accd7f09d3', name='numTrees', doc='Number of trees to train (>= 1).'): 20},\n  0.8872232007414572),\n ({Param(parent='RandomForestClassifier_22accd7f09d3', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5,\n   Param(parent='RandomForestClassifier_22accd7f09d3', name='numTrees', doc='Number of trees to train (>= 1).'): 100},\n  0.8882009830669134),\n ({Param(parent='RandomForestClassifier_22accd7f09d3', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 10,\n   Param(parent='RandomForestClassifier_22accd7f09d3', name='numTrees', doc='Number of trees to train (>= 1).'): 10},\n  0.9048881178918161),\n ({Param(parent='RandomForestClassifier_22accd7f09d3', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 10,\n   Param(parent='RandomForestClassifier_22accd7f09d3', name='numTrees', doc='Number of trees to train (>= 1).'): 20},\n  0.9142624641849961),\n ({Param(parent='RandomForestClassifier_22accd7f09d3', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 10,\n   Param(parent='RandomForestClassifier_22accd7f09d3', name='numTrees', doc='Number of trees to train (>= 1).'): 100},\n  0.918062552946504)]"
     ]
    }
   ],
   "source": [
    "cv_model = pipeline_model.stages[-1]\n",
    "rf_model = cv_model.bestModel\n",
    "\n",
    "list(zip(cv_model.getEstimatorParamMaps(), cv_model.avgMetrics))\n",
    "\n",
    "#print(rf_model.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34111888-ff10-4392-b8e8-1521639ee3cc",
     "showTitle": false,
     "title": "--i18n-11e6c47a-ddb1-416d-92a5-2f61340f9a5e"
    }
   },
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54b4eee7-329a-4751-9fcb-a8d7b9281cf0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bedrooms</td>\n",
       "      <td>0.156562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>room_typeIndex</td>\n",
       "      <td>0.145999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>accommodates</td>\n",
       "      <td>0.145594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neighbourhood_cleansedIndex</td>\n",
       "      <td>0.094439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>beds</td>\n",
       "      <td>0.074897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>host_total_listings_count</td>\n",
       "      <td>0.056620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>latitude</td>\n",
       "      <td>0.051551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>longitude</td>\n",
       "      <td>0.039338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>review_scores_rating</td>\n",
       "      <td>0.033169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>number_of_reviews</td>\n",
       "      <td>0.031997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>minimum_nights</td>\n",
       "      <td>0.029923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bathrooms</td>\n",
       "      <td>0.028360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>property_typeIndex</td>\n",
       "      <td>0.027096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cancellation_policyIndex</td>\n",
       "      <td>0.016465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>review_scores_cleanliness</td>\n",
       "      <td>0.013874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>review_scores_location</td>\n",
       "      <td>0.009767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>review_scores_accuracy</td>\n",
       "      <td>0.008065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>host_is_superhostIndex</td>\n",
       "      <td>0.007745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>instant_bookableIndex</td>\n",
       "      <td>0.006643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>review_scores_value</td>\n",
       "      <td>0.006014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>review_scores_communication</td>\n",
       "      <td>0.003827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>review_scores_checkin</td>\n",
       "      <td>0.002303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>review_scores_rating_na</td>\n",
       "      <td>0.001572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bed_typeIndex</td>\n",
       "      <td>0.001396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>review_scores_accuracy_na</td>\n",
       "      <td>0.001294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>review_scores_communication_na</td>\n",
       "      <td>0.001186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>review_scores_checkin_na</td>\n",
       "      <td>0.001121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>review_scores_value_na</td>\n",
       "      <td>0.001037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>review_scores_location_na</td>\n",
       "      <td>0.000928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>review_scores_cleanliness_na</td>\n",
       "      <td>0.000905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>bathrooms_na</td>\n",
       "      <td>0.000222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>bedrooms_na</td>\n",
       "      <td>0.000047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>beds_na</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature</th>\n      <th>importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>12</th>\n      <td>bedrooms</td>\n      <td>0.156562</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>room_typeIndex</td>\n      <td>0.145999</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>accommodates</td>\n      <td>0.145594</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>neighbourhood_cleansedIndex</td>\n      <td>0.094439</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>beds</td>\n      <td>0.074897</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>host_total_listings_count</td>\n      <td>0.056620</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>latitude</td>\n      <td>0.051551</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>longitude</td>\n      <td>0.039338</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>review_scores_rating</td>\n      <td>0.033169</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>number_of_reviews</td>\n      <td>0.031997</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>minimum_nights</td>\n      <td>0.029923</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>bathrooms</td>\n      <td>0.028360</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>property_typeIndex</td>\n      <td>0.027096</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cancellation_policyIndex</td>\n      <td>0.016465</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>review_scores_cleanliness</td>\n      <td>0.013874</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>review_scores_location</td>\n      <td>0.009767</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>review_scores_accuracy</td>\n      <td>0.008065</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>host_is_superhostIndex</td>\n      <td>0.007745</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>instant_bookableIndex</td>\n      <td>0.006643</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>review_scores_value</td>\n      <td>0.006014</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>review_scores_communication</td>\n      <td>0.003827</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>review_scores_checkin</td>\n      <td>0.002303</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>review_scores_rating_na</td>\n      <td>0.001572</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>bed_typeIndex</td>\n      <td>0.001396</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>review_scores_accuracy_na</td>\n      <td>0.001294</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>review_scores_communication_na</td>\n      <td>0.001186</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>review_scores_checkin_na</td>\n      <td>0.001121</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>review_scores_value_na</td>\n      <td>0.001037</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>review_scores_location_na</td>\n      <td>0.000928</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>review_scores_cleanliness_na</td>\n      <td>0.000905</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>bathrooms_na</td>\n      <td>0.000222</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>bedrooms_na</td>\n      <td>0.000047</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>beds_na</td>\n      <td>0.000045</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pandas_df = pd.DataFrame(list(zip(vec_assembler.getInputCols(), rf_model.featureImportances)), columns=[\"feature\", \"importance\"])\n",
    "top_features = pandas_df.sort_values([\"importance\"], ascending=False)\n",
    "top_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40456f27-1c49-4344-8576-ec179d5e725b",
     "showTitle": false,
     "title": "--i18n-ae7e312e-d32b-4b02-97ff-ad4d2c737892"
    }
   },
   "source": [
    "Do those features make sense? Would you use those features when picking an Airbnb rental?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b215381-f8f5-4c89-9f8d-fa0a41fe2a12",
     "showTitle": false,
     "title": "--i18n-950eb40f-b1d2-4e7f-8b07-76faff6b8186"
    }
   },
   "source": [
    "## Apply Model to Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4131feb0-8318-4ba1-a448-20372833e176",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC is 0.92\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "pred_df = pipeline_model.transform(test_df)\n",
    "area_under_roc = evaluator.evaluate(pred_df)\n",
    "\n",
    "print(f\"Area under ROC is {area_under_roc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c48810da-920a-4ef4-9b70-4c996e26d256",
     "showTitle": false,
     "title": "--i18n-01974668-f242-4b8a-ac80-adda3b98392d"
    }
   },
   "source": [
    "## Save Model\n",
    "\n",
    "Save the model to **`DA.paths.working_dir`** (variable defined in Classroom-Setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3a3f0d3-9d0a-497d-a456-aaba3e242b45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline.model.write().overwrite().save(DA.paths.working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b396e57f-89e2-4cd9-8296-95795d0afb72",
     "showTitle": false,
     "title": "--i18n-f5fdf1a9-2a65-4252-aa76-18807dbb3a9d"
    }
   },
   "source": [
    "## Sklearn vs SparkML\n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\" target=\"_blank\">Sklearn RandomForestRegressor</a> vs <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.RandomForestRegressor.html?highlight=randomfore#pyspark.ml.regression.RandomForestRegressor\" target=\"_blank\">SparkML RandomForestRegressor</a>.\n",
    "\n",
    "Look at these params in particular:\n",
    "* **n_estimators** (sklearn) vs **numTrees** (SparkML)\n",
    "* **max_depth** (sklearn) vs **maxDepth** (SparkML)\n",
    "* **max_features** (sklearn) vs **featureSubsetStrategy** (SparkML)\n",
    "* **maxBins** (SparkML only)\n",
    "\n",
    "What do you notice that is different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a727044c-3b31-451e-a61a-16810f5ad215",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "camelCase for Spark\n",
    "no maxbins for sparkml."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fba9821f-449f-4817-b056-23eab977b8b5",
     "showTitle": false,
     "title": "--i18n-a2c7fb12-fd0b-493f-be4f-793d0a61695b"
    }
   },
   "source": [
    "## Classroom Cleanup\n",
    "\n",
    "Run the following cell to remove lessons-specific assets created during this lesson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5105d401-1cf0-4f6b-9687-fbcca2bb8190",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e0831c9-dcf0-4998-8793-37cd1e91a32b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "&copy; 2023 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ML 07L - Hyperparameter Tuning Lab",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
