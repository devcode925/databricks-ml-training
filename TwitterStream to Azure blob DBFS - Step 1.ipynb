{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cd822eb-8bac-4918-8faa-2239fd33ed58",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# TwitterStream to Azure blob / DBFS - Step 1\n",
    "\n",
    "\n",
    "* [jump to Twitter-Stream-Azure notebook]($./TwitterStream to Azure blob DBFS - Step 1)\n",
    "* [jump to Twitter-SentimentAnalysis notebook]($./Huggingface Sentiment Analysis Step 3)\n",
    "* [Pipeline](https://adb-3234447377967728.8.azuredatabricks.net/?o=3234447377967728#joblist/pipelines/c7029259-25b6-4c56-83bd-ca0b5254db9c/updates/8035cda4-bf92-4cb5-896a-12e94ac36f3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "823de530-58ad-4690-9d50-fb611b9e6323",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Mount ADLS Gen2 or Blob Storage with ABFS ## https://docs.databricks.com/dbfs/mounts.html#mount-adls-gen2-or-blob-storage-with-abfs\n",
    "# first create a keyvault, then generate a secret.\n",
    "# create mnt here holder dbutils.fs.mkdirs(\"/mnt/tweet-holder\")\n",
    "#https://docs.databricks.com/security/aad-storage-service-principal.html\n",
    "# create a blob to hold the tweets.\n",
    "## for a principal you need to set the scope. for this one it's https://adb-3234447377967728.8.azuredatabricks.net/?o=3234447377967728#secrets/createScope otherwise it's\n",
    "## Go to https://<databricks-instance>#secrets/createScope \n",
    "# dbutils.secrets.listScopes() to see the name in case you forget.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40769b9d-f7ef-43ff-a6f6-13981de725f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[1]: [SecretScope(name='azure-storage')]"
     ]
    }
   ],
   "source": [
    "#dbutils.secrets.listScopes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aebfab6e-8110-4fe7-b95e-75f60604242b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounts successfully refreshed.\nOut[37]: [MountInfo(mountPoint='/databricks-datasets', source='databricks-datasets', encryptionType=''),\n MountInfo(mountPoint='/databricks/mlflow-tracking', source='databricks/mlflow-tracking', encryptionType=''),\n MountInfo(mountPoint='/databricks-results', source='databricks-results', encryptionType=''),\n MountInfo(mountPoint='/databricks/mlflow-registry', source='databricks/mlflow-registry', encryptionType=''),\n MountInfo(mountPoint='/mnt/tweet-holder', source='abfss://twittercatcher@twitterblobber.dfs.core.windows.net/', encryptionType=''),\n MountInfo(mountPoint='/', source='DatabricksRoot', encryptionType='')]"
     ]
    }
   ],
   "source": [
    "#dbutils.fs.refreshMounts()\n",
    "#dbutils.fs.mkdirs(\"/mnt/tweet-holder\")\n",
    "#dbutils.fs.mounts()\n",
    "# client id for configs is the applicationid in for my service principal xxx-ebf417f68948"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63c235b2-f942-4e57-8a7c-e2415c717a19",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-- for databricks file system(dbfs) https://learn.microsoft.com/en-us/azure/databricks/dbfs/mounts\n",
    "  \n",
    "  -- need to create service principal https://learn.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal\n",
    "\n",
    "-- create database connector https://portal.azure.com/?quickstart=true#view/HubsExtension/BrowseResource/resourceType/Microsoft.Databricks%2FaccessConnectors\n",
    "\n",
    "\n",
    "xxxx66d-6509-4d5d-xxx-cac8fa836db0\n",
    "  \"fs.azure.account.oauth2.client.secret\": dbutils.secrets.get(scope=\"azure-storage\",key=\"blob-storage-databricks\"),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "xxx-62ad-41fe-9f06-4eb1643f73ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mExecutionError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-304508422698303>:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# depreciated and didn't work\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m dbutils\u001B[38;5;241m.\u001B[39mfs\u001B[38;5;241m.\u001B[39mmount(\n\u001B[1;32m      3\u001B[0m source \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwasbs://twittercatcher@twitterblobber.blob.core.windows.net\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      4\u001B[0m mount_point \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/mnt/tweet-holder2\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      5\u001B[0m extra_configs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.azure.account.key.twitterblobber.blob.core.windows.net\u001B[39m\u001B[38;5;124m\"\u001B[39m:dbutils\u001B[38;5;241m.\u001B[39msecrets\u001B[38;5;241m.\u001B[39mget(scope\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mazure-storage\u001B[39m\u001B[38;5;124m\"\u001B[39m,key\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mblob-storage-databricks\u001B[39m\u001B[38;5;124m\"\u001B[39m)})\n\u001B[1;32m      7\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mtext(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/mnt/tweet-holder2/test.json\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      9\u001B[0m df\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py:362\u001B[0m, in \u001B[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    360\u001B[0m exc\u001B[38;5;241m.\u001B[39m__context__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    361\u001B[0m exc\u001B[38;5;241m.\u001B[39m__cause__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 362\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n\n\u001B[0;31mExecutionError\u001B[0m: An error occurred while calling o435.mount.\n: shaded.databricks.org.apache.hadoop.fs.azure.AzureException: hadoop_azure_shaded.com.microsoft.azure.storage.StorageException: Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.\n\tat shaded.databricks.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.retrieveMetadata(AzureNativeFileSystemStore.java:2297)\n\tat shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem.getFileStatus(NativeAzureFileSystem.java:2380)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.verifyAzureFileSystem(DBUtilsCore.scala:1118)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:988)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1036)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:550)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:645)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:666)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:407)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:405)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:402)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:68)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:450)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:435)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:68)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:640)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:559)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:68)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:550)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:520)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:68)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:132)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1030)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: hadoop_azure_shaded.com.microsoft.azure.storage.StorageException: Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.\n\tat hadoop_azure_shaded.com.microsoft.azure.storage.StorageException.translateException(StorageException.java:87)\n\tat hadoop_azure_shaded.com.microsoft.azure.storage.core.StorageRequest.materializeException(StorageRequest.java:305)\n\tat hadoop_azure_shaded.com.microsoft.azure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:196)\n\tat hadoop_azure_shaded.com.microsoft.azure.storage.blob.CloudBlobContainer.downloadAttributes(CloudBlobContainer.java:545)\n\tat shaded.databricks.org.apache.hadoop.fs.azure.StorageInterfaceImpl$CloudBlobContainerWrapperImpl.downloadAttributes(StorageInterfaceImpl.java:265)\n\tat shaded.databricks.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.checkContainer(AzureNativeFileSystemStore.java:1242)\n\tat shaded.databricks.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.retrieveMetadata(AzureNativeFileSystemStore.java:2215)\n\t... 36 more\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mExecutionError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-304508422698303>:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# depreciated and didn't work\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m dbutils\u001B[38;5;241m.\u001B[39mfs\u001B[38;5;241m.\u001B[39mmount(\n\u001B[1;32m      3\u001B[0m source \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwasbs://twittercatcher@twitterblobber.blob.core.windows.net\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      4\u001B[0m mount_point \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/mnt/tweet-holder2\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      5\u001B[0m extra_configs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.azure.account.key.twitterblobber.blob.core.windows.net\u001B[39m\u001B[38;5;124m\"\u001B[39m:dbutils\u001B[38;5;241m.\u001B[39msecrets\u001B[38;5;241m.\u001B[39mget(scope\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mazure-storage\u001B[39m\u001B[38;5;124m\"\u001B[39m,key\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mblob-storage-databricks\u001B[39m\u001B[38;5;124m\"\u001B[39m)})\n\u001B[1;32m      7\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mtext(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/mnt/tweet-holder2/test.json\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      9\u001B[0m df\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py:362\u001B[0m, in \u001B[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    360\u001B[0m exc\u001B[38;5;241m.\u001B[39m__context__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    361\u001B[0m exc\u001B[38;5;241m.\u001B[39m__cause__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 362\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n\n\u001B[0;31mExecutionError\u001B[0m: An error occurred while calling o435.mount.\n: shaded.databricks.org.apache.hadoop.fs.azure.AzureException: hadoop_azure_shaded.com.microsoft.azure.storage.StorageException: Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.\n\tat shaded.databricks.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.retrieveMetadata(AzureNativeFileSystemStore.java:2297)\n\tat shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem.getFileStatus(NativeAzureFileSystem.java:2380)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.verifyAzureFileSystem(DBUtilsCore.scala:1118)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:988)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1036)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:550)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:645)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:666)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:407)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:405)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:402)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:68)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:450)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:435)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:68)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:640)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:559)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:68)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:550)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:520)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:68)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:132)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1030)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: hadoop_azure_shaded.com.microsoft.azure.storage.StorageException: Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.\n\tat hadoop_azure_shaded.com.microsoft.azure.storage.StorageException.translateException(StorageException.java:87)\n\tat hadoop_azure_shaded.com.microsoft.azure.storage.core.StorageRequest.materializeException(StorageRequest.java:305)\n\tat hadoop_azure_shaded.com.microsoft.azure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:196)\n\tat hadoop_azure_shaded.com.microsoft.azure.storage.blob.CloudBlobContainer.downloadAttributes(CloudBlobContainer.java:545)\n\tat shaded.databricks.org.apache.hadoop.fs.azure.StorageInterfaceImpl$CloudBlobContainerWrapperImpl.downloadAttributes(StorageInterfaceImpl.java:265)\n\tat shaded.databricks.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.checkContainer(AzureNativeFileSystemStore.java:1242)\n\tat shaded.databricks.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.retrieveMetadata(AzureNativeFileSystemStore.java:2215)\n\t... 36 more\n",
       "errorSummary": "shaded.databricks.org.apache.hadoop.fs.azure.AzureException: hadoop_azure_shaded.com.microsoft.azure.storage.StorageException: Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "# depreciated and didn't work\n",
    "dbutils.fs.mount(\n",
    "source = \"wasbs://twittercatcher@twitterblobber.blob.core.windows.net\",\n",
    "mount_point = \"/mnt/tweet-holder2\",\n",
    "extra_configs = {\"fs.azure.account.key.twitterblobber.blob.core.windows.net\":dbutils.secrets.get(scope=\"azure-storage\",key=\"blob-storage-databricks\")})\n",
    "\n",
    "df = spark.read.text(\"/mnt/tweet-holder/test.json\")\n",
    "\n",
    "df.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "xxx-6633-47dc-a0d3-09696efc7f1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[17]: True"
     ]
    }
   ],
   "source": [
    "''' created the first time -- #used service principal, and secret, scopt didn't work for me.\n",
    "configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "          \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "          \"fs.azure.account.oauth2.client.id\": \"xxxx-b46c-402d-bed0-xx\",\n",
    "          \"fs.azure.account.oauth2.client.secret\": \"xxxx\",\n",
    "          \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/xxx-eced-43d3-b1f2-7315f4ec788c/oauth2/token\"}\n",
    "dbutils.fs.mount( source = \"abfss://twittercatcher@twitterblobber.dfs.core.windows.net/\", mount_point = \"/mnt/tweet-holder\", extra_configs = configs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "xx-d83c-4674-8202-9ca367960771",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# should use databricks secrets and the CLI to store and retrieve those keys in a safe way.\n",
    "#\n",
    "# for a first try, you can setup you twitter keys here\n",
    "consumer_key = \"xxx\"\n",
    "consumer_secret = \"xxxxZInFbkPHTxGftPi8cW0bODuogUmgWXtGpPfncT61j\"\n",
    "access_token = \"xxx8955956233-8mU4smuEPIOCne5uqoX9jJB3R6mHOt\"\n",
    "access_token_secret = \"bPNPvtCFuhAxNJJvlzMmaPlEDkUX3UzYaLz4HfXLxa289\"\n",
    "\n",
    "# in my demo, I read in the keys from another notebook in the cell below (which can be savely removed or commented out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "xxx-9e00-47e4-90d4-b9f9f9aa0804",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Notebook not found: Repos/emailchuckjr@gmail.com/delta-live-tables-notebooks/twitter-dlt-huggingface-demo/Twitter-Setup. Notebooks can be specified via a relative path (./Notebook or ../folder/Notebook) or via an absolute path (/Abs/Path/to/Notebook). Make sure you are specifying the path correctly.\n\nStacktrace:\n  /Repos/emailchuckjr@gmail.com/delta-live-tables-notebooks/twitter-dlt-huggingface-demo/Twitter-Stream-S3: python",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %run \"./Twitter-Setup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e765cd8-db78-4125-84e5-b23215820926",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/databricks/python3/bin/python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d11dfd4d-990b-41f0-948b-7553f48faf2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy\r\n  Downloading tweepy-4.13.0-py3-none-any.whl (102 kB)\r\n\u001B[?25l\r\u001B[K     |███▏                            | 10 kB 35.8 MB/s eta 0:00:01\r\u001B[K     |██████▍                         | 20 kB 38.7 MB/s eta 0:00:01\r\u001B[K     |█████████▋                      | 30 kB 48.2 MB/s eta 0:00:01\r\u001B[K     |████████████▊                   | 40 kB 55.7 MB/s eta 0:00:01\r\u001B[K     |████████████████                | 51 kB 60.1 MB/s eta 0:00:01\r\u001B[K     |███████████████████▏            | 61 kB 65.5 MB/s eta 0:00:01\r\u001B[K     |██████████████████████▎         | 71 kB 42.1 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████▌      | 81 kB 40.4 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████▊   | 92 kB 42.8 MB/s eta 0:00:01\r\u001B[K     |███████████████████████████████▉| 102 kB 45.8 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████████| 102 kB 45.8 MB/s \r\n\u001B[?25hCollecting jsonpickle\r\n  Downloading jsonpickle-3.0.1-py2.py3-none-any.whl (40 kB)\r\n\u001B[?25l\r\u001B[K     |████████                        | 10 kB 51.9 MB/s eta 0:00:01\r\u001B[K     |████████████████▏               | 20 kB 60.7 MB/s eta 0:00:01\r\u001B[K     |████████████████████████▎       | 30 kB 72.2 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████████| 40 kB 6.6 MB/s \r\n\u001B[?25hRequirement already satisfied: oauthlib<4,>=3.2.0 in /databricks/python3/lib/python3.9/site-packages (from tweepy) (3.2.0)\r\nRequirement already satisfied: requests-oauthlib<2,>=1.2.0 in /databricks/python3/lib/python3.9/site-packages (from tweepy) (1.3.1)\r\nRequirement already satisfied: requests<3,>=2.27.0 in /databricks/python3/lib/python3.9/site-packages (from tweepy) (2.27.1)\r\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy) (3.3)\r\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy) (2.0.4)\r\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy) (1.26.9)\r\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy) (2021.10.8)\r\nInstalling collected packages: tweepy, jsonpickle\r\nSuccessfully installed jsonpickle-3.0.1 tweepy-4.13.0\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 23.0.1 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-a4e9e473-f623-43f7-8d61-f85ae879992e/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy jsonpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "926a51f0-9ff0-4ed4-8263-f333f35fd794",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter screen name: devworkStill\nretrieving tweet 1: RT @AndreaR9Md: It's legal to shoot their mama, you wanna tell the or should I?\nretrieving tweet 2: @willystizzle4 @colinsteinke @bshallenberger @Seth_3773 Really interesting how the Cubs are moving last years short… https://t.co/xHG5Ll0S6X\nretrieving tweet 3: Probably my favorite pic of @CoachPainter     Until the day he holds the natty trophy #boilerup #gocubs\nretrieving tweet 4: RT @iftiinkaguriga: Some people really don’t fear Yawm al Qiyamah and it shows\nretrieving tweet 5: RT @SarahHuckabee: Amazing day with my family and @ARGameandFish helping protect our bear population. Holding baby bear 🐻 cubs is unforgett…\nretrieving tweet 6: @4Sullivan @bshallenberger @Seth_3773 Really interesting how the Cubs are moving last years shortstop Nico Hoerner… https://t.co/KNTTF8Giur\nretrieving tweet 7: @AndreaR9Md The cubs look about as happy as her kids do.  Is this supposed to show her softer side?\nretrieving tweet 8: RT @CubsViews: Seiya Suzuki’s Opening Day Status Becomes Clear #ChicagoCubs #Cubbies #Cubs #CubsNation #CubsFans https://t.co/VF4xNyALUS\nretrieving tweet 9: RT @MLBPipeline: Owen Caissie tacks on another for Canada!\n\nThe @Cubs' No. 13 prospect blasts a homer in his #WorldBaseballClassic debut. h…\nretrieving tweet 10: RT @SarahHuckabee: Amazing day with my family and @ARGameandFish helping protect our bear population. Holding baby bear 🐻 cubs is unforgett…\nWrote local file  /dbfs/mnt/tweet-holder/tweets_1678670934.json\nretrieving tweet 11: RT @333donna: @Cubs Fergie Jenkins signing for the kids this morning. #SpringTraining2023 https://t.co/pTFmIcUJzR\nretrieving tweet 12: RT @OhRick4: Let me guess...\n\nYou shot the mother.\nretrieving tweet 13: @333donna @Cubs Love you Fergie.\nretrieving tweet 14: @TheNFBC DC Draft Update:\n\n20.295: 1B/2B/3B Isaac Paredes (@RaysBaseball)\n21.306: P Noah Syndergaard (@Dodgers)\n22.… https://t.co/3IxlLMHaGo\nretrieving tweet 15: RT @fergieajenkins: Always 😉✌🏾\nretrieving tweet 16: RT @OhRick4: Let me guess...\n\nYou shot the mother.\nretrieving tweet 17: Shop #Cubs  Phone Cases, Frames, Puzzles, Prints, Towels and more @ &gt;&gt; https://t.co/QQFDbBXlvD &lt;&lt; #mlb #BaseBall… https://t.co/OOC0f7EwS7\nretrieving tweet 18: From yesterday to today. ❄️ to 🌵\n\nWhat can be described as a very adventure filled trip south yesterday, was happy… https://t.co/89HQiraNOT\nretrieving tweet 19: RT @stevesher_7: Another spring training goodbye.  Thank you @onemillioncubs for your company and your friendship.  Enjoy the rest of your…\nretrieving tweet 20: Why do people love comparing themselves to animals and stripping themselves of humanity? It's weird. In that case a… https://t.co/6LBx3XNOD3\nWrote local file  /dbfs/mnt/tweet-holder/tweets_1678671041.json\nretrieving tweet 21: Hanging out with Cubs prospects in the hotel lobby https://t.co/jFUncqBF4A\nretrieving tweet 22: Young Chicago Cubs fans today will never get to experience the view of fading asphalt fields and abandoned donut sh… https://t.co/otRBY1KJaz\nretrieving tweet 23: RT @RonFilipkowski: The bear cubs are wondering what they did to deserve this great honor. https://t.co/eA5OTQfqSa\nretrieving tweet 24: RT @myleshiggins: Watch out everyone. Stay safe!\nretrieving tweet 25: RT @_agharass: Adult lions pretend that the bites of the cubs hurt them to encourage the little ones - by Yann Arthus-Bertrand (1946), Fren…\nretrieving tweet 26: RT @JonesForAR: @SarahHuckabee @ARGameandFish Cute bear cubs!\nretrieving tweet 27: 3 Cubs prospects they can deal for top talent right now\nhttps://t.co/AWcI3Lz9RZ\nretrieving tweet 28: The best part of Republican women?\n\nNO FUCKING PENIS!\nretrieving tweet 29: RT @kanzaimtiaz: \"It bodes well that the country’s best player has taken 3 of the nation’s finest under his wing — a 28-year-old mama bear…\nretrieving tweet 30: @pca_2024 @Cubs But #Cubs nation is losing their shit over Wesneski’s 8 innings pitched LMAO. https://t.co/P7vMxx2THu\nWrote local file  /dbfs/mnt/tweet-holder/tweets_1678671185.json\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tweepy.streaming:Stream encountered an exception\nTraceback (most recent call last):\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-a4e9e473-f623-43f7-8d61-f85ae879992e/lib/python3.9/site-packages/tweepy/streaming.py\", line 94, in _connect\n    self.on_data(line)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-a4e9e473-f623-43f7-8d61-f85ae879992e/lib/python3.9/site-packages/tweepy/streaming.py\", line 418, in on_data\n    return self.on_status(status)\n  File \"<command-2294717165822538>\", line 42, in on_status\n    raise Exception(\"Finished job\")\nException: Finished job\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded tweets  30\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import calendar\n",
    "import time\n",
    "import jsonpickle\n",
    "import sys\n",
    "\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, timeout=60)\n",
    "print(f'Twitter screen name: {api.verify_credentials().screen_name}')\n",
    "\n",
    "\n",
    "# Subclass Stream  \n",
    "class TweetStream(tweepy.Stream):\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        tweepy.Stream.__init__(self, consumer_key=consumer_key, consumer_secret=consumer_secret,\n",
    "                             access_token=access_token, access_token_secret=access_token_secret)\n",
    "        self.filename = filename\n",
    "        self.text_count = 0\n",
    "        self.tweet_stack = []\n",
    "\n",
    "\n",
    "    def on_status(self, status):\n",
    "        #print('*'+status.text)\n",
    "        self.text_count = self.text_count + 1\n",
    "        self.tweet_stack.append(status)\n",
    "    \n",
    "        # when to print\n",
    "        if (self.text_count % 1 == 0):\n",
    "            print(f'retrieving tweet {self.text_count}: {status.text}')\n",
    "\n",
    "        # how many tweets to batch into one file\n",
    "        if (self.text_count % 10 == 0):\n",
    "            self.write_file()\n",
    "            self.tweet_stack = []\n",
    "\n",
    "        # hard exit after collecting n tweets\n",
    "        if (self.text_count == 30):\n",
    "            raise Exception(\"Finished job\")\n",
    "\n",
    "    def write_file(self):\n",
    "        file_timestamp = calendar.timegm(time.gmtime())\n",
    "        fname = self.filename + '/tweets_' + str(file_timestamp) + '.json'\n",
    "\n",
    "\n",
    "        f = open(fname, 'w')\n",
    "        for tweet in self.tweet_stack:\n",
    "            f.write(jsonpickle.encode(tweet._json, unpicklable=False) + '\\n')\n",
    "        f.close()\n",
    "        print(\"Wrote local file \", fname)\n",
    "\n",
    "    def on_error(self, status_code):\n",
    "        print(\"Error with code \", status_code)\n",
    "        sys.exit()\n",
    "\n",
    "\n",
    "# Initialize instance of the subclass\n",
    "tweet_stream = TweetStream(\"/dbfs/mnt/tweet-holder\")\n",
    "\n",
    "# Filter realtime Tweets by keyword\n",
    "try:\n",
    "    tweet_stream.filter(languages=[\"en\"],\n",
    "                        track=[\"Cub Foods\", \"Cub food\"\n",
    "                               \"Minnesota Grocer\"])\n",
    "\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"some error \", e)\n",
    "    print(\"Writing out tweets file before I have to exit\")\n",
    "    tweet_stream.write_file()\n",
    "finally:\n",
    "    print(\"Downloaded tweets \", tweet_stream.text_count)\n",
    "    tweet_stream.disconnect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88186616-fa3f-4f29-834c-17bc00a548e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"stop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19af5176-3d77-478c-900e-a02634905576",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Setup Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b1cfbfa-918e-4f18-bf78-997fc2411544",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### create a DBFS directory, check for number of files, deletes a certain number of files ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4bf623f-8ac6-4cc6-aa55-802481b86c6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/dbfs/data/tweets’: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "# create a directory to buffer the streamed data\n",
    "!mkdir \"/dbfs/mnt/tweet-holder/tweets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba5b1fea-6798-434b-b30d-e83c5496aef7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# count files in that directory - compare with #files in DLT bronze\n",
    "#!ls -l /dbfs/mnt/tweet-holder/twitter_dataeng2 | wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4ccbc30-8fca-4e05-90c8-aadad0328e13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# disk usage\n",
    "#!du -h  dbfs/mnt/tweet-holder/twitter_dataeng2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "777d3170-0fb3-4cce-b086-0abc3308986a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''# remove n files. use this to trim demo\n",
    "files = dbutils.fs.ls(\"/data/twitter_dataeng2\")\n",
    "del = 400\n",
    "print(f'number of files: {len(files)}')\n",
    "print(f'number of files to delete: {del}')\n",
    "\n",
    "\n",
    "for x, file in enumerate(files):\n",
    "  # delete n files from directory\n",
    "  if x < del :\n",
    "    # print(x, file)\n",
    "    dbutils.fs.rm(file.path)\n",
    "\n",
    "    \n",
    "# use dbutils to copy over files... \n",
    "# dbutils.fs.cp(\"/data/twitter_dataeng/\" +f, \"/data/twitter_dataeng2/\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "TwitterStream to Azure blob DBFS - Step 1",
   "notebookOrigID": 2294717165822532,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
